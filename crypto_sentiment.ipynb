{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 12 - Tales from the Crypto\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sentiment Analysis\n",
    "\n",
    "Use the [newsapi](https://newsapi.org/) to pull the latest news articles for Bitcoin and Ethereum and create a DataFrame of sentiment scores for each coin.\n",
    "\n",
    "Use descriptive statistics to answer the following questions:\n",
    "1. Which coin had the highest mean positive score?\n",
    "2. Which coin had the highest negative score?\n",
    "3. Which coin had the highest positive score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\jackc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import nltk as nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "import nltk\n",
    "nltk.download()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read your api key environment variable\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"newsAPI\")\n",
    "type(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a newsapi client\n",
    "from newsapi import NewsApiClient\n",
    "newsapi = NewsApiClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the Bitcoin news articles\n",
    "bitcoin_news_articles = newsapi.get_everything(q='bitcoin', language=\"en\", sort_by=\"relevancy\")\n",
    "bitcoin_news_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the Ethereum news articles\n",
    "ethereum_news_articles = newsapi.get_everything(q='ethereum', language=\"en\", sort_by=\"relevancy\")\n",
    "ethereum_news_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Bitcoin sentiment scores DataFrame\n",
    "bitcoin_sentiments = []\n",
    "\n",
    "for article in bitcoin_news_articles[\"articles\"]:\n",
    "    try:\n",
    "        text = article[\"content\"]\n",
    "        date = article[\"publishedAt\"][:10]\n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "        \n",
    "        bitcoin_sentiments.append({\n",
    "            \"text\": text,\n",
    "            \"date\": date,\n",
    "            \"compound\": compound,\n",
    "            \"positive\": pos,\n",
    "            \"negative\": neg,\n",
    "            \"neutral\": neu\n",
    "            \n",
    "        })\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "bitcoin_df = pd.DataFrame(bitcoin_sentiments)\n",
    "cols = [\"date\", \"text\", \"compound\", \"positive\", \"negative\", \"neutral\"]\n",
    "bitcoin_df = bitcoin_df[cols]\n",
    "\n",
    "bitcoin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Ethereum sentiment scores DataFrame\n",
    "ethereum_sentiments = []\n",
    "\n",
    "for article in ethereum_news_articles[\"articles\"]:\n",
    "    try:\n",
    "        text = article[\"content\"]\n",
    "        date = article[\"publishedAt\"][:10]\n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "        \n",
    "        ethereum_sentiments.append({\n",
    "            \"text\": text,\n",
    "            \"date\": date,\n",
    "            \"compound\": compound,\n",
    "            \"positive\": pos,\n",
    "            \"negative\": neg,\n",
    "            \"neutral\": neu\n",
    "            \n",
    "        })\n",
    "        \n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "ethereum_df = pd.DataFrame(ethereum_sentiments)\n",
    "cols = [\"date\", \"text\", \"compound\", \"positive\", \"negative\", \"neutral\"]\n",
    "ethereum_df = ethereum_df[cols]\n",
    "\n",
    "ethereum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the Bitcoin Sentiment\n",
    "bitcoin_df.describe()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the Ethereum Sentiment\n",
    "ethereum_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "Q: Which coin had the highest mean positive score?\n",
    "\n",
    "A: \n",
    "\n",
    "Q: Which coin had the highest compound score?\n",
    "\n",
    "A: \n",
    "\n",
    "Q. Which coin had the highest positive score?\n",
    "\n",
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Natural Language Processing\n",
    "---\n",
    "###   Tokenizer\n",
    "\n",
    "In this section, you will use NLTK and Python to tokenize the text for each coin. Be sure to:\n",
    "1. Lowercase each word.\n",
    "2. Remove Punctuation.\n",
    "3. Remove Stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from string import punctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a list of stopwords\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "# Expand the default stopwords list if necessary\n",
    "sw_addons = {'said', 'sent', 'found', 'including', 'today', 'announced', 'week', 'basically', 'also'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the tokenizer function\n",
    "def clean_text(article):\n",
    "    \n",
    "    # Create a list of the words\n",
    "    sw = set(stopwords.words('english'))\n",
    "\n",
    "    # Convert the words to lowercase\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "\n",
    "    # Remove the punctuation\n",
    "    re_clean = regex.sub('', article)\n",
    "\n",
    "    # Remove the stop words\n",
    "    words = word_tokenize(re_clean)\n",
    "\n",
    "    # Lemmatize Words into root words\n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # .union combines all items from two sets (AND EXCLUDES DUPLICATES)\n",
    "    output = [word.lower() for word in lem if word.lower() not in sw.union(sw_addons)]\n",
    "    \n",
    "    # Return the final list\n",
    "    return output\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for Bitcoin\n",
    "bitcoin_df['tokens'] = bitcoin_df['text'].apply(word_tokenize)\n",
    "bitcoin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for Ethereum\n",
    "ethereum_df['tokens'] = ethereum_df['text'].apply(word_tokenize)\n",
    "ethereum_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGrams and Frequency Analysis\n",
    "\n",
    "In this section you will look at the ngrams and word frequency for each coin. \n",
    "\n",
    "1. Use NLTK to produce the n-grams for N = 2. \n",
    "2. List the top 10 words for each coin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Bitcoin N-grams where N=2\n",
    "def word_counter(corpus): \n",
    "    # Combine all articles in corpus into one large string\n",
    "    big_string = ' '.join(corpus.text)\n",
    "    processed = clean_text(big_string)\n",
    "    bigrams = ngrams(processed, n=2)\n",
    "    top_10 = dict(Counter(bigrams).most_common(10))\n",
    "    return pd.DataFrame(list(top_10.items()), columns=['bigram', 'count'])\n",
    "\n",
    "word_counter(bitcoin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Ethereum N-grams where N=2\n",
    "word_counter(ethereum_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function token_count generates the top 10 words for a given coin\n",
    "def token_count(tokens, N=10):\n",
    "    \"\"\"Returns the top N tokens from the frequency count\"\"\"\n",
    "    return Counter(tokens).most_common(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use token_count to get the top 10 words for Bitcoin\n",
    "bitcoin_text = ' '.join(bitcoin_df.text)\n",
    "bitcoin_processed = clean_text(bitcoin_text)\n",
    "token_count(bitcoin_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use token_count to get the top 10 words for Ethereum\n",
    "ethereum_text = ' '.join(ethereum_df.text)\n",
    "ethereum_processed = clean_text(ethereum_text)\n",
    "\n",
    "token_count(ethereum_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds\n",
    "\n",
    "In this section, you will generate word clouds for each coin to summarize the news for each coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = [20.0, 10.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Bitcoin word cloud\n",
    "bitcoin_words = ' '.join(bitcoin_processed)\n",
    "\n",
    "bitcoin_cloud = WordCloud().generate(bitcoin_words)\n",
    "plt.imshow(bitcoin_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Ethereum word cloud\n",
    "ethereum_words = ' '.join(ethereum_processed)\n",
    "\n",
    "ethereum_cloud = WordCloud().generate(ethereum_words)\n",
    "plt.imshow(ethereum_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Named Entity Recognition\n",
    "\n",
    "In this section, you will build a named entity recognition model for both Bitcoin and Ethereum, then visualize the tags using SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the language model for SpaCy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Bitcoin NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all of the Bitcoin text together\n",
    "bitcoin_text = ' '.join(bitcoin_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the NER processor on all of the text\n",
    "bitcoin_ner = nlp(bitcoin_text)\n",
    "\n",
    "# Add a title to the document\n",
    "bitcoin_ner.user_data[\"title\"] = \"Bitcoin NER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the visualization\n",
    "displacy.render(btc_ner, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all Entities\n",
    "entity = []\n",
    "label = []\n",
    "\n",
    "for ent in btc_ner.ents:\n",
    "    entity.append(ent.text)\n",
    "    label.append(ent.label_)\n",
    "    \n",
    "data = {\"Entity\": entity, \"Label\":label}\n",
    "\n",
    "bitcoin_entities = pd.DataFrame(data)\n",
    "bitcoin_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethereum NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all of the Ethereum text together\n",
    "ethereum_text = ' '.join(ethereum_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the NER processor on all of the text\n",
    "ethereum_ner = nlp(ethereum_text)\n",
    "\n",
    "# Add a title to the document\n",
    "ethereum_ner.user_data[\"title\"] = \"Ethereum NER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the visualization\n",
    "displacy.render(ethereum_ner, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ethereum NER\n",
    "A multi-billion dollar cryptocurrency company has apologised to users after its sale of metaverse land sparked a frenzy that temporarily brought down the Ethereum ORG cryptocurrency. Yuga Labs PERSON , the comp… [+3475 chars] When Bored Ape Yacht Club PERSON creators Yuga Labs PERSON announced its Otherside ORG NFT collection would launch on April 30 DATE , it was predicted by many to be the biggest NFT ORG launch ever. Otherside is an upcoming Bore… [+6669 chars] If you ever wanted to buy an NFT ORG based on Ethereum ORG , you would have to pay a transaction fee to register it on the blockchain. Last week DATE , that fee skyrocketed to unprecedented levels. So you might ha… [+7216 chars] Digital ORG transformation is disrupting global businesses. The legal industry must consider digital technologies to keep changing tech trends. Blockchain-based smart contracts are one CARDINAL example of such te… [+8816 chars] Mike Masnick PERSON wrote a good piece on this topic on his Techdirt blog last week DATE . I particularly like this part: First ORDINAL , lets look at the world without any content moderation. A website that has no cont… [+3109 chars] We are excited to bring Transform ORG 2022 back in-person July 19 DATE and virtually July 20 - 28 DATE . Join AI and data leaders for insightful talks and exciting networking opportunities. Register today DATE ! For ove… [+6087 chars] The U.S. Treasury Department ORG has implicated the North Korea GPE -backed Lazarus Group ORG (aka Hidden Cobra PERSON ) in the theft of $540 million MONEY from video game Axie Infinity's GPE Ronin Network ORG last month DATE . On Thursday DATE … [+5464 chars] Did you miss a session from GamesBeat Summit 2022? All sessions are available to stream now. Learn more.  Jam City GPE is betting big on its first ORDINAL blockchain game, Champions Ascension, and today DATE it dis… [+14217 chars] If youre a current cryptocurrency hodler or a potential investor in India GPE , the government wants you to think twice before putting your money in the sector. For cryptocurrency enthusiasts in the coun… [+7600 chars] This past weekend DATE Bored & ORG amp; Hungry ORG , the world’s Bored Ape Yacht Club restaurant opened in Long Beach GPE , California GPE . Ahead of its opening, the unique concept announced via Twitter PRODUCT that it would not o… [+2021 chars] NFTs are undoubtedly not a fad. Since the explosion of these digital tokens, many projects have been developed which have broadened their utility. But while some of these projects have recorded trem… [+20480 chars] What happened  The cryptocurrency market had another bad end to the week DATE on Friday DATE , selling off into what could be a wild weekend DATE . Yuga Labs PERSON will be minting some form of land for its metaverse and t… [+2413 chars] As the competition between blockchains for developers and users picks up, investors have to decide which cryptocurrency is a likely winner in the future. Ethereum( ETH 1.81% PERCENT ) has long had a big lead… [+2983 chars] During market sell-offs, sometimes it can be difficult to decide what to invest in. Do you buy growth stocks on sale? Maybe you want a dividend stock you can count on for passive income. Or maybe you… [+5931 chars] They grow up so fast! The Ethereum ORG ( ETH 0.60% PERCENT ) cryptocurrency was conceived in 2013 DATE , and the first ORDINAL proper transactions took place two years later DATE . The Ethereum ORG blockchain has rolled out 14 CARDINAL importan… [+3620 chars] 2021 DATE was the year DATE of Ethereum ( ETH 0.08% PERCENT ). The proof-of-stake cryptocurrency stormed the charts, gaining 400% PERCENT to become a legitimate challenger to Bitcoin's ( BTC 0.06% PERCENT ) crypto supremacy. The NF ORG … [+3478 chars] Solana(SOL 0.61% PERCENT ) was one CARDINAL of the hottest cryptocurrencies of 2021 DATE , emerging from relative obscurity and evolving into a crypto that Bank of America ORG called \"the Visa of the digital asset ecosystem\" by… [+2974 chars] Late last month DATE , airBaltic ORG presented the Planies ORG , the airline’s latest entry into the world of non-fungible tokens (NFTs). The Latvian NORP airline has placed high expectations on this collection of ether… [ +6907 NORP chars] If you don't have much of an interest in the crypto or NFT (non-fungible token) space, you may find Gucci PERSON 's decision to accept payments in cryptocurrency to be strange. However, the brand already has… [+1449 chars] Since 2014 DATE , when Vitalik Buterin PERSON helped to create Ethereum ORG ( ETH 0.95% PERCENT ), he has known there was a problem: scalability. In Ethereum GPE 's white paper (the document that explains how the system is suppose… [+4771 chars]\n",
    "# List all Entities\n",
    "entity = []\n",
    "label = []\n",
    "\n",
    "for ent in eth_ner.ents:\n",
    "    entity.append(ent.text)\n",
    "    label.append(ent.label_)\n",
    "    \n",
    "data = {\"Entity\": entity, \"Label\":label}\n",
    "\n",
    "ethereum_entities = pd.DataFrame(data)\n",
    "ethereum_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
